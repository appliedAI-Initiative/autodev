import logging
import os
from abc import ABC, abstractmethod
from typing import Union, Optional

from optimum.onnxruntime import ORTModelForCausalLM
from peft import PeftModel
from transformers import AutoModelForCausalLM, PreTrainedModel, PreTrainedTokenizer, AutoTokenizer, AutoConfig

log = logging.getLogger(__name__)


def model_id_to_fn(model_id: str):
    return model_id.replace("/", "--")


def model_id_from_fn(model_fn: str):
    return model_fn.replace("--", "/")


class ModelTransformation(ABC):
    @abstractmethod
    def transform(self, model: AutoModelForCausalLM):
        pass

    def automodel_kwargs(self) -> dict:
        return {}


class ModelTransformationBitsAndBytes8Bit(ModelTransformation):
    """
    Enables the 8-bit transformation from the bitsandbytes library (which must be installed).
    """
    def transform(self, model: AutoModelForCausalLM):
        return model

    def automodel_kwargs(self) -> dict:
        return dict(load_in_8bit=True, device_map="auto")


class ModelTransformationBetterTransformer(ModelTransformation):
    def transform(self, model: PreTrainedModel):
        from optimum.bettertransformer import BetterTransformer
        return BetterTransformer.transform(model)


class ModelFactory:
    def __init__(self, base_model_id: str, transformation: Optional[ModelTransformation] = None):
        self.base_model_id = base_model_id
        self.transformation = transformation

    def create_tokenizer(self) -> PreTrainedTokenizer:
        return AutoTokenizer.from_pretrained(self.base_model_id, trust_remote_code=True)

    def create_model(self,
            model_path: Optional[str] = None,
            transformation: Optional[ModelTransformation] = None
        ) -> Union[PreTrainedModel, PeftModel, ORTModelForCausalLM]:
        """
        :param model_path: a path to a directory containing the model/checkpoint to load or
            a path/identifier known to the transformers library. Specifically, the path can be one of the
            following:
                * a model identifier/path that is known to the HuggingFace Hub
                * a directory containing a persisted checkpoint, which may contain either a regular pretrained model
                  or a PEFT model (in which case adapter_config.json must be present in the directory)
                * a directory containing an ONNX model as generated by ONNXConverter (in which case model.onnx
                  must be present)
            If not specified, the base model will be loaded.
        :param transformation: an optional model transformation (which overrides any transformation that was
            given at construction)
        :return: the model
        """
        transformation = transformation if transformation is not None else self.transformation
        automodel_kwargs = {} if transformation is None else transformation.automodel_kwargs()

        is_dir = os.path.isdir(model_path)
        if is_dir and os.path.exists(os.path.join(model_path, "adapter_config.json")):
            log.info(f"Loading base model '{self.base_model_id}'")
            base_model = AutoModelForCausalLM.from_pretrained(self.base_model_id, trust_remote_code=True,
                **automodel_kwargs)
            log.info(f"Loading PEFT model from {model_path}")
            model = PeftModel.from_pretrained(base_model, model_path)
        elif is_dir and os.path.exists(os.path.join(model_path, "model.onnx")):
            onnx_path = os.path.join(model_path, "model.onnx")
            log.info(f"Loading ONNX model from {onnx_path}")
            decoder_session, decoder_with_past_session = ORTModelForCausalLM.load_model(onnx_path,
                decoder_with_past_path=None, provider="CPUExecutionProvider")
            config = AutoConfig.from_pretrained(model_path)
            model = ORTModelForCausalLM(decoder_session, config, [onnx_path], use_cache=False,
                use_io_binding=False)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, **automodel_kwargs)

        if transformation is not None:
            model = transformation.transform(model)

        return model


class SantaCoderModelFactory(ModelFactory):
    def __init__(self):
        super().__init__(base_model_id="bigcode/santacoder")



